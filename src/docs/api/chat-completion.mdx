---
title: Chat Completion API
description: Generate AI responses with the OpenRouter Chat Completion API
---

# Chat Completion API

The Chat Completion API is the core endpoint for generating responses from AI models. It follows a format similar to OpenAI's Chat API but provides access to a wider range of models.

## Endpoint

```
POST https://api.openrouter.ai/v1/chat/completions
```

## Request Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `model` | string | Yes | The model identifier (e.g., `anthropic/claude-3-opus`, `openai/gpt-4o`) |
| `messages` | array | Yes | An array of message objects representing the conversation |
| `temperature` | number | No | Controls randomness (0-1, default varies by model) |
| `max_tokens` | integer | No | Maximum number of tokens to generate |
| `top_p` | number | No | Alternative to temperature for nucleus sampling (0-1) |
| `stream` | boolean | No | Whether to stream the response (default: false) |
| `stop` | string or array | No | Sequences where the API will stop generating |
| `frequency_penalty` | number | No | Penalizes frequent tokens (-2 to 2, default: 0) |
| `presence_penalty` | number | No | Penalizes repeated tokens (-2 to 2, default: 0) |
| `functions` | array | No | Function specifications for function calling |
| `function_call` | string or object | No | Controls function calling behavior |
| `response_format` | object | No | Format specification for the response |
| `seed` | integer | No | Random seed for deterministic results |

## Message Format

The `messages` parameter is an array of objects with the following structure:

```json
{
  "role": "user" | "assistant" | "system",
  "content": "The message content",
  "name": "optional name for the message sender"
}
```

- `role`: Can be "system" (instructions to the AI), "user" (user messages), or "assistant" (previous AI responses)
- `content`: The text content of the message
- `name`: Optional identifier for the message sender (useful in multi-user contexts)

## Basic Example

```javascript
const response = await fetch('https://api.openrouter.ai/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${OPENROUTER_API_KEY}`,
    'Content-Type': 'application/json',
    'HTTP-Referer': 'https://your-app.com'
  },
  body: JSON.stringify({
    model: 'anthropic/claude-3-opus',
    messages: [
      {role: 'system', content: 'You are a helpful assistant.'},
      {role: 'user', content: 'What is the capital of France?'}
    ],
    temperature: 0.7,
    max_tokens: 500
  })
});

const data = await response.json();
console.log(data.choices[0].message.content);
```

## Response Format

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1677858242,
  "model": "anthropic/claude-3-opus",
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "The capital of France is Paris."
      },
      "finish_reason": "stop",
      "index": 0
    }
  ],
  "usage": {
    "prompt_tokens": 25,
    "completion_tokens": 7,
    "total_tokens": 32
  },
  "openrouter": {
    "provider": "anthropic"
  }
}
```

## Streaming Responses

Set `stream: true` to receive responses as they are generated:

```javascript
const response = await fetch('https://api.openrouter.ai/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${OPENROUTER_API_KEY}`,
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    model: 'openai/gpt-4o',
    messages: [{role: 'user', content: 'Write a poem about APIs'}],
    stream: true
  })
});

// Process the stream
const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const { done, value } = await reader.read();
  if (done) break;
  
  const chunk = decoder.decode(value);
  // Process each chunk that begins with "data: "
  const lines = chunk.split('\n').filter(line => line.startsWith('data: '));
  
  for (const line of lines) {
    if (line === 'data: [DONE]') continue;
    const data = JSON.parse(line.substring(6));
    
    // Extract and use the content
    const content = data.choices[0]?.delta?.content || '';
    console.log(content);
  }
}
```

## Function Calling

Function calling allows the model to invoke functions defined in your request:

```javascript
const response = await fetch('https://api.openrouter.ai/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${OPENROUTER_API_KEY}`,
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    model: 'openai/gpt-4o',
    messages: [{role: 'user', content: 'What\'s the weather in New York?'}],
    functions: [
      {
        name: 'get_weather',
        description: 'Get the current weather in a location',
        parameters: {
          type: 'object',
          properties: {
            location: {
              type: 'string',
              description: 'The city and state, e.g. New York, NY'
            }
          },
          required: ['location']
        }
      }
    ],
    function_call: 'auto'
  })
});

const data = await response.json();
if (data.choices[0].message.function_call) {
  const functionCall = data.choices[0].message.function_call;
  console.log(`Function: ${functionCall.name}`);
  console.log(`Arguments: ${functionCall.arguments}`);
  
  // You would then call your actual function with these arguments
}
```

## Error Handling

If your request fails, you'll receive an error response:

```json
{
  "error": {
    "message": "Description of what went wrong",
    "type": "invalid_request_error",
    "param": "model",
    "code": 400
  }
}
```

See [Error Handling](/docs/api/errors) for more details.

## Next Steps

- [Explore available Models](/docs/models)
- [Learn about API Authentication](/docs/api/authentication)
- [Understand Error Handling](/docs/api/errors)
