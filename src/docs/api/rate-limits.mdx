---
title: Rate Limits
description: Understanding and working with OpenRouter API rate limits
---

# Rate Limits

OpenRouter implements rate limits to ensure fair usage, maintain platform stability, and prevent abuse. This guide explains how rate limits work and how to handle them in your applications.

## Rate Limit Types

OpenRouter applies several types of rate limits:

### Request-Based Limits

| Plan | Requests per Minute (RPM) | Requests per Day |
|------|---------------------------|------------------|
| Free | 60 | 5,000 |
| Pro | 120 | 50,000 |
| Business | 300 | 200,000 |
| Enterprise | Custom | Custom |

### Token-Based Limits

Token limits vary by model and plan:

| Plan | Input Tokens per Minute | Output Tokens per Minute |
|------|-------------------------|--------------------------|
| Free | 20,000 | 10,000 |
| Pro | 100,000 | 50,000 |
| Business | 500,000 | 250,000 |
| Enterprise | Custom | Custom |

### Concurrent Request Limits

The number of simultaneous requests allowed:

| Plan | Concurrent Requests |
|------|---------------------|
| Free | 5 |
| Pro | 20 |
| Business | 50 |
| Enterprise | Custom |

## Rate Limit Headers

Rate limit information is included in response headers:

```
x-ratelimit-limit-requests: 60
x-ratelimit-remaining-requests: 45
x-ratelimit-reset-requests: 1686142558

x-ratelimit-limit-tokens: 100000
x-ratelimit-remaining-tokens: 86420
x-ratelimit-reset-tokens: 1686142558
```

Header explanations:

- `x-ratelimit-limit-*`: Maximum number of requests or tokens allowed in the current window
- `x-ratelimit-remaining-*`: Number of requests or tokens remaining in the current window
- `x-ratelimit-reset-*`: Unix timestamp when the rate limit window resets

## Handling Rate Limits

When you exceed a rate limit, the API returns a `429 Too Many Requests` status code with a response like:

```json
{
  "error": {
    "type": "rate_limit_exceeded",
    "message": "Rate limit exceeded: 60 requests per minute. Please retry after 24 seconds.",
    "param": null,
    "reset_at": 1686142558
  }
}
```

### Best Practices for Rate Limit Handling

1. **Implement Exponential Backoff**

```javascript
async function makeRequestWithBackoff(url, options, maxRetries = 5) {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      const response = await fetch(url, options);
      
      if (response.status === 429) {
        const data = await response.json();
        const resetTime = data.error.reset_at;
        const waitTime = resetTime ? (resetTime - Math.floor(Date.now() / 1000)) : Math.pow(2, attempt) + Math.random();
        
        console.log(`Rate limited. Retrying in ${waitTime} seconds...`);
        await new Promise(resolve => setTimeout(resolve, waitTime * 1000));
        continue;
      }
      
      return response;
    } catch (error) {
      if (attempt === maxRetries - 1) throw error;
      
      const waitTime = Math.pow(2, attempt) + Math.random();
      await new Promise(resolve => setTimeout(resolve, waitTime * 1000));
    }
  }
}
```

2. **Monitor Rate Limit Headers**

```javascript
function checkRateLimits(response) {
  const remainingRequests = parseInt(response.headers.get('x-ratelimit-remaining-requests') || '0');
  const remainingTokens = parseInt(response.headers.get('x-ratelimit-remaining-tokens') || '0');
  
  if (remainingRequests < 10) {
    console.warn(`Warning: Only ${remainingRequests} requests remaining in current window`);
  }
  
  if (remainingTokens < 5000) {
    console.warn(`Warning: Only ${remainingTokens} tokens remaining in current window`);
  }
  
  return response;
}

// Usage
fetch(url, options)
  .then(checkRateLimits)
  .then(processResponse);
```

3. **Implement Request Queuing**

For high-volume applications, implement a request queue:

```javascript
class RequestQueue {
  constructor() {
    this.queue = [];
    this.processing = false;
  }
  
  async add(requestFn) {
    return new Promise((resolve, reject) => {
      this.queue.push({
        request: requestFn,
        resolve,
        reject
      });
      
      if (!this.processing) {
        this.processQueue();
      }
    });
  }
  
  async processQueue() {
    if (this.queue.length === 0) {
      this.processing = false;
      return;
    }
    
    this.processing = true;
    const { request, resolve, reject } = this.queue.shift();
    
    try {
      const result = await request();
      resolve(result);
    } catch (error) {
      reject(error);
    }
    
    // Add delay between requests to avoid hitting rate limits
    await new Promise(resolve => setTimeout(resolve, 100));
    this.processQueue();
  }
}

// Usage
const queue = new RequestQueue();
queue.add(() => makeRequest('/api/endpoint-1'));
queue.add(() => makeRequest('/api/endpoint-2'));
```

## Optimizing API Usage

### Token Usage Optimization

1. **Use Model Context Efficiently**
   - Only include relevant context in your requests
   - Summarize long conversations when possible
   - Consider using embeddings for long-term memory

2. **Batch Related Requests**
   - Combine multiple related queries into a single request
   - Process multiple items in one call when appropriate
   - Use function calls to return structured data efficiently

3. **Choose Appropriate Models**
   - Use smaller models for simpler tasks
   - Reserve high-capability models for complex requests
   - Consider using model routing based on task complexity

### Request Patterns

1. **Streaming for Long Responses**
   - Use streaming to start processing responses immediately
   - This can reduce perceived latency for users

2. **Caching Common Responses**
   - Implement client-side caching for repetitive queries
   - Consider semantic caching for similar questions

3. **Graceful Degradation**
   - Design your application to handle reduced functionality when rate limits approach
   - Implement priority queues for critical vs. non-critical requests

## Rate Limit Increases

If you need higher rate limits:

1. **Upgrade Your Plan**
   - Consider upgrading to a higher tier plan for increased limits

2. **Request Custom Limits**
   - Enterprise customers can request custom rate limit configurations
   - Contact our [support team](/support) with your specific requirements

3. **Optimize First**
   - Before requesting increases, ensure you've optimized your current usage
   - We may ask for usage patterns to suggest optimization strategies

## Model-Specific Rate Limits

Some high-demand models have additional rate limits:

| Model | Additional Limits |
|-------|-------------------|
| OpenAI GPT-4o | 100 RPM on Pro plan |
| Anthropic Claude 3 Opus | 30 RPM on Pro plan |
| Gemini 1.5 Pro | 60 RPM on Pro plan |

## Monitoring Your Usage

1. **Dashboard**
   - Monitor your usage in real-time via the [OpenRouter dashboard](/dashboard)
   - Set up alerts for approaching limits

2. **Usage API**
   - Track programmatic usage via the Usage API
   - Endpoint: `GET /v1/usage`
   - Example response:
   ```json
   {
     "usage": {
       "requests": {
         "total": 1250,
         "limit": 5000,
         "remaining": 3750
       },
       "tokens": {
         "input": {
           "total": 15000,
           "limit": 100000,
           "remaining": 85000
         },
         "output": {
           "total": 8000,
           "limit": 50000,
           "remaining": 42000
         }
       }
     },
     "period": {
       "start": "2023-06-01T00:00:00Z",
       "end": "2023-06-02T00:00:00Z"
     }
   }
   ```

## Next Steps

- [Explore Error Handling](/docs/api/errors) for comprehensive API reliability
- [Learn about Versioning Policy](/docs/api/versioning) to future-proof your integration
- [Implement Cost Management](/docs/guides/cost-management) strategies
