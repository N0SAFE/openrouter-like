---
title: Streaming Responses
description: How to use streaming capabilities with OpenRouter API
---

# Streaming Responses

OpenRouter supports streaming responses for real-time text generation. This is particularly useful for chatbots, creative writing assistants, and any application where displaying incremental results improves the user experience.

## How Streaming Works

Streaming allows you to receive the model's output as it's being generated, rather than waiting for the complete response:

1. Your application sends a request with `stream: true`
2. The API begins returning chunks of the response as they're generated
3. Each chunk contains a portion of the generated text
4. A final chunk signals the completion of the response

## Enabling Streaming

To enable streaming, simply set the `stream` parameter to `true` in your API request:

```json
{
  "model": "anthropic/claude-3-opus",
  "messages": [
    {"role": "user", "content": "Write a short story about AI"}
  ],
  "stream": true
}
```

## Response Format

Streaming responses follow the Server-Sent Events (SSE) format. Each chunk is prefixed with `data: ` and contains a JSON object with the partial completion:

```
data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1678861374,"model":"anthropic/claude-3-opus","choices":[{"delta":{"role":"assistant","content":"Once"},"index":0,"finish_reason":null}]}

data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1678861374,"model":"anthropic/claude-3-opus","choices":[{"delta":{"content":" upon"},"index":0,"finish_reason":null}]}

data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1678861374,"model":"anthropic/claude-3-opus","choices":[{"delta":{"content":" a"},"index":0,"finish_reason":null}]}

data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1678861374,"model":"anthropic/claude-3-opus","choices":[{"delta":{"content":" time"},"index":0,"finish_reason":null}]}

data: [DONE]
```

## Client Implementation

### JavaScript Example

```javascript
const response = await fetch('https://api.openrouter.ai/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': `Bearer ${apiKey}`
  },
  body: JSON.stringify({
    model: 'anthropic/claude-3-opus',
    messages: [
      {role: 'user', content: 'Write a short story about AI'}
    ],
    stream: true
  })
});

const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const {value, done} = await reader.read();
  if (done) break;
  
  // Process the chunk
  const chunk = decoder.decode(value);
  const lines = chunk.split('\n').filter(line => line.startsWith('data: ') && line !== 'data: [DONE]');
  
  for (const line of lines) {
    const data = JSON.parse(line.substring(6));
    const content = data.choices[0].delta.content;
    if (content) {
      // Append content to your UI
      console.log(content);
    }
  }
}
```

### Python Example

```python
import requests
import json

response = requests.post(
    "https://api.openrouter.ai/v1/chat/completions",
    headers={
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    },
    json={
        "model": "anthropic/claude-3-opus",
        "messages": [
            {"role": "user", "content": "Write a short story about AI"}
        ],
        "stream": True
    },
    stream=True
)

for line in response.iter_lines():
    if line:
        line_text = line.decode('utf-8')
        if line_text.startswith('data: '):
            if line_text == 'data: [DONE]':
                break
            
            data = json.loads(line_text[6:])  # Remove 'data: ' prefix
            content = data['choices'][0]['delta'].get('content', '')
            if content:
                # Process the content
                print(content, end='', flush=True)
```

## Benefits of Streaming

- **Improved User Experience**: Users see responses forming in real-time rather than waiting for complete responses
- **Faster Perceived Response Time**: First words appear quickly, giving the impression of speed
- **Smooth Interface**: Creates a more natural, conversational feel in chat applications
- **Early Termination**: Allows clients to cancel generation early if needed
- **Enhanced Interactivity**: Enables building more responsive applications

## Error Handling

When using streaming, you should implement proper error handling:

- Check for HTTP status codes indicating errors
- Handle network interruptions gracefully
- Provide fallback mechanisms for when streaming fails
- Consider implementing retry logic with exponential backoff

## Performance Considerations

Streaming can affect your application's performance in several ways:

- **Connection Overhead**: Maintaining an open connection requires resources
- **Processing Overhead**: Processing many small chunks can be more CPU-intensive
- **Buffering**: Consider implementing client-side buffering for smoother display
- **Connection Limits**: Be aware of browser connection limits for web applications

## Model Support

All models supported by OpenRouter can handle streaming, but performance characteristics may vary:

- Some models produce tokens more consistently than others
- Output speed can vary between model providers
- Larger models may have slightly slower initial response times

## Next Steps

- [Learn about Rate Limits](/docs/api/rate-limits)
- [Explore Function Calling](/docs/api/function-calling)
- [Implement Error Handling](/docs/api/errors)
