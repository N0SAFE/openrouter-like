---
title: Load Balancing
description: Distribute traffic across multiple models for optimal performance and reliability with OpenRouter
---

# Load Balancing

OpenRouter provides built-in load balancing capabilities to distribute your AI requests across multiple models and providers, improving reliability, optimizing costs, and enhancing performance.

## Overview

Load balancing allows you to:

- **Improve reliability** by automatically routing around unavailable models
- **Optimize costs** by distributing traffic based on pricing
- **Enhance performance** by selecting models with lower latency
- **Balance quality** by routing more important requests to better models

## Load Balancing Strategies

OpenRouter supports several load balancing strategies:

### 1. Fallback Chains

Configure a sequence of models to try if the primary model is unavailable:

```json
{
  "model": [
    "anthropic/claude-3-sonnet",
    "openai/gpt-4o",
    "google/gemini-1.5-pro",
    "openai/gpt-3.5-turbo"
  ],
  "messages": [
    {"role": "user", "content": "Hello, world!"}
  ]
}
```

The API will try each model in sequence until one succeeds.

### 2. Weighted Distribution

Distribute traffic across models based on specified weights:

```json
{
  "model": {
    "anthropic/claude-3-sonnet": 0.6,
    "openai/gpt-4o": 0.3,
    "openai/gpt-3.5-turbo": 0.1
  },
  "messages": [
    {"role": "user", "content": "Hello, world!"}
  ]
}
```

This distributes 60% of requests to Claude, 30% to GPT-4o, and 10% to GPT-3.5.

### 3. Conditional Routing

Route requests to different models based on content, metadata, or other conditions:

```json
{
  "route": {
    "default": "openai/gpt-3.5-turbo",
    "conditions": [
      {
        "if": "contains(message, 'code') OR contains(message, 'programming')",
        "then": "openai/gpt-4o"
      },
      {
        "if": "length(message) > 1000",
        "then": "anthropic/claude-3-opus"
      }
    ]
  },
  "messages": [
    {"role": "user", "content": "Write a Java function to sort a list"}
  ]
}
```

### 4. Performance-Based Routing

Automatically route to the model with the best current performance:

```json
{
  "model": {
    "selection": "fastest",
    "options": [
      "openai/gpt-4o",
      "anthropic/claude-3-sonnet",
      "google/gemini-1.5-pro"
    ]
  },
  "messages": [
    {"role": "user", "content": "Hello, world!"}
  ]
}
```

Options for `selection` include:
- `fastest` - Choose the model with lowest current latency
- `cheapest` - Choose the lowest-cost model
- `best` - Balance between quality, cost, and performance

## Implementing Load Balancing

### Using the API Directly

```javascript
async function queryWithLoadBalancing() {
  const response = await fetch("https://api.openrouter.ai/v1/chat/completions", {
    method: "POST",
    headers: {
      "Authorization": `Bearer ${process.env.OPENROUTER_API_KEY}`,
      "Content-Type": "application/json"
    },
    body: JSON.stringify({
      "model": [
        "anthropic/claude-3-sonnet",
        "openai/gpt-4o",
        "openai/gpt-3.5-turbo"
      ],
      "messages": [
        {"role": "user", "content": "What is the capital of France?"}
      ]
    })
  });
  
  return await response.json();
}
```

### Using the JavaScript SDK

```javascript
import { OpenRouter } from "openrouter";

const openrouter = new OpenRouter({
  apiKey: process.env.OPENROUTER_API_KEY
});

const response = await openrouter.chat.completions.create({
  model: {
    selection: "fastest",
    options: [
      "anthropic/claude-3-sonnet",
      "openai/gpt-4o",
      "google/gemini-1.5-pro"
    ]
  },
  messages: [
    { role: "user", content: "What is the capital of France?" }
  ]
});
```

### Using the Python SDK

```python
from openrouter import OpenRouter

openrouter = OpenRouter(api_key="your_api_key")

response = openrouter.chat.completions.create(
    model={
        "anthropic/claude-3-sonnet": 0.7,
        "openai/gpt-4o": 0.3,
    },
    messages=[
        {"role": "user", "content": "What is the capital of France?"}
    ]
)
```

## Load Balancing Configuration

### Global Configuration

You can configure default load balancing settings in your OpenRouter dashboard:

1. Navigate to **Settings** > **API Configuration**
2. Select the **Load Balancing** tab
3. Configure your default fallbacks and routing preferences

### Request-Level Configuration

Load balancing settings specified in individual API requests override your global configuration.

## Advanced Configuration

### Health Checks

OpenRouter automatically performs health checks on providers and models:

```json
{
  "model": [
    "anthropic/claude-3-sonnet",
    "openai/gpt-4o",
    "openai/gpt-3.5-turbo"
  ],
  "health_checks": {
    "frequency": "high",
    "timeout": 2000
  },
  "messages": [
    {"role": "user", "content": "Hello, world!"}
  ]
}
```

Health check options:
- `frequency`: `low` (default), `medium`, or `high`
- `timeout`: Milliseconds to wait before trying the next model

### Circuit Breaking

Automatically detect failing models and temporarily remove them from rotation:

```json
{
  "model": [
    "anthropic/claude-3-sonnet",
    "openai/gpt-4o",
    "openai/gpt-3.5-turbo"
  ],
  "circuit_breaker": {
    "threshold": 3,
    "recovery_time": 60
  },
  "messages": [
    {"role": "user", "content": "Hello, world!"}
  ]
}
```

Settings:
- `threshold`: Number of failures before removing a model (default: 5)
- `recovery_time`: Seconds to wait before retrying (default: 30)

### Response Metadata

When using load balancing, responses include metadata about the selected model:

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1677858242,
  "model": "anthropic/claude-3-sonnet",
  "routing": {
    "selected": "anthropic/claude-3-sonnet",
    "attempted": ["anthropic/claude-3-sonnet"],
    "selection_strategy": "fallback_chain"
  },
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "The capital of France is Paris."
      },
      "finish_reason": "stop",
      "index": 0
    }
  ]
}
```

## Best Practices

### Performance Optimization

1. **Geographic Routing** - Consider the user's location when selecting models
2. **Request Characteristics** - Route complex requests to more capable models
3. **Time Sensitivity** - Use faster models for time-sensitive applications
4. **Caching** - Implement caching for common requests to reduce load

### Cost Optimization

1. **Cost-Aware Routing** - Route to less expensive models for simpler tasks
2. **Traffic Shaping** - Route a percentage of traffic to cheaper models
3. **Smart Fallbacks** - Use a cost-optimization strategy for fallbacks

### Reliability

1. **Provider Diversification** - Distribute across multiple providers
2. **Model Diversification** - Include models of different architectures
3. **Persistent Connections** - Maintain connections for faster switching
4. **Graceful Degradation** - Fall back to simpler models rather than failing

## Common Scenarios

### High-Availability Production System

```json
{
  "model": [
    "anthropic/claude-3-sonnet",
    "openai/gpt-4o",
    "google/gemini-1.5-pro",
    "openai/gpt-3.5-turbo"
  ],
  "circuit_breaker": {
    "threshold": 2,
    "recovery_time": 120
  },
  "health_checks": {
    "frequency": "high",
    "timeout": 1500
  },
  "messages": [
    {"role": "user", "content": "Hello, world!"}
  ]
}
```

### Cost-Optimized System

```json
{
  "model": {
    "openai/gpt-3.5-turbo": 0.7,
    "anthropic/claude-instant": 0.2,
    "google/gemini-1.0-pro": 0.1
  },
  "route": {
    "default": "openai/gpt-3.5-turbo",
    "conditions": [
      {
        "if": "complexity(message) > 0.8",
        "then": "anthropic/claude-3-sonnet"
      }
    ]
  },
  "messages": [
    {"role": "user", "content": "Hello, world!"}
  ]
}
```

## Monitoring and Analytics

OpenRouter provides detailed analytics on your load balancing performance:

1. **Model Distribution** - See how traffic is distributed across models
2. **Fallback Rates** - Monitor how often fallbacks are triggered
3. **Response Times** - Compare latency across different models
4. **Error Rates** - Track errors by model and provider
5. **Cost Savings** - Calculate savings from smart routing

Access these metrics in your OpenRouter dashboard under **Analytics** > **Load Balancing**.

## Next Steps

- [Custom Endpoints](/docs/advanced-features/custom-endpoints) - Create specialized endpoints with preset routing rules
- [Caching Strategies](/docs/advanced-features/caching) - Combine load balancing with caching for better performance
- [Analytics & Logging](/docs/advanced-features/analytics) - Monitor your load balancing performance
- [High Availability](/docs/deployment/high-availability) - Build resilient systems with OpenRouter
